+++
date = "2022-05-04T21:05:33+05:30"
title = "Research"
+++

## Statement

A search for the creative possibilities of new technologies has been at the core of my scientific and artistic research in my academic and professional career. In my early graduate research, I combined human-computer interaction, physical computing, and music information retrieval techniques to develop novel gestural interfaces to control sound synthesis processes that I then used expressively in my creative practice. Currently, I am exploring the creative affordances of the machine learning paradigm in music composition and performance. I develop, adapt, compose, and perform with machine learning techniques and tools that learn models from music encoded in a symbolic format. In my current research, I am investigating the expressive possibilities of machine learning-assisted music-making directly in the audio domain.


The contemporary and rapidly changing new music economy landscape requires researchers, professionals, and makers not only aware of the new tools and techniques to design and implement new creative output, but also to think critically on the biases and implications of using data-driven technologies in creative practice in general, and music-making in particular. As a researcher, scholar, computer musician, and performer, these are for me important topics and a focal point. As a result, in my scientific research and creative output, I interrogate and explore the aesthetic possibilities of new technologies, but I also pay attention to their omissions, silences, and disconnections. Hence, I am committed to an inclusive approach to the development of new ways of producing, researching, and enjoying music.

## Publications

### 2025

**Vigliensoni, G.**, and R. Fiebrink. 2025. Data- and interaction-driven approaches for sustained musical practices with machine learning. _Journal of New Music Research_, January, 1–14. [doi:10.1080/09298215.2024.2442361](https://doi.org/10.1080/09298215.2024.2442361).


Bryan-Kinns, N., S. Zheng, F. Castro, M. Lewis, and J. Chang, **G. Vigliensoni**, T. Broad, M. P. Clemens, and E. Wilson. 2025. XAIxArts Manifesto: Explainable AI for the Arts. In _Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems_. [doi:10.1145/3706599.3716227](https://arxiv.org/abs/2502.21220)

Ford, C., E. Wilson, S. Zheng, **G. Vigliensoni**, J. Rezwana, L. Xiao, M. Clemens, M. Lewis, D. Hemment, A. Chamberlain, H. Kennedy, and N. Bryan-Kinns. Explainable AI for the Arts 3 (XAIxArts2). In _Proceedings of the ACM Creativity and Cognition Conference (C&C ’25)_.

Nickel, V., and **G. Vigliensoni**. 2025. Sonic lead: A survey of sound-first games. To be published in _Proceedings of the Digital Games Research Association (DiGRA)_.

Bryan-Kinns, N., A. Wszeborowska, O. Sutskova, E. Wilson, P. Perry, R. Fiebrink, **G. Vigliensoni**, R. Lindell, A. Coronel, and N. Correia. 2025. Responsible and ethical use of AI for music: The value of small datasets. To be published in _Proceedings of AM.ICAD 2025 (Audio Mostly + International Conference on Auditory Display: Joint Conference)_.

Nguyen, V., and **G. Vigliensoni**. 2025. Embedding visual thinking into an AI-driven furniture design critiquing system. To be published in _Proceedings of the International Conference on Computational Creativity (ICCC)_.

<!-- Nguyen, V., and **G. Vigliensoni**. 2025. fCrit: A visual explanation system for furniture design creative support. Submitted to Explainable AI for the Arts 3 (XAIxArts 2025). -->



### 2024

Tecks, A., T. Peschlow, and **G. Vigliensoni**. 2024. Explainability paths for sustained artistic practice. In _Proceedings of the the Second International Workshop on eXplainable AI for the Arts at the ACM Creativity and Cognition Conference (XAIxArts2024)_.  [doi.org/10.48550/arxiv.2407.15216](https://doi.org/10.48550/arxiv.2407.15216)

Bryan-Kinns, N., C. Ford, S. Zheng, H. Kennedy, A. Chamberlain, M. Lewis, D. Hemment, Z. Li, Q. Wu, L. Xiao, G. Xia, J. Rezwana, M. Clemens, and **G. Vigliensoni**. Explainable AI for the Arts 2 (XAIxArts2). In _Proceedings of the ACM Creativity and Cognition Conference (C&C ’24)_. [doi.org/10.1145/3635636.3660763](https://doi.org/10.1145/3635636.3660763 )

### 2023

**Vigliensoni, G.**, and R. Fiebrink. 2023. Steering latent audio models through interactive machine learning. In _Proceedings of the 14th International Conference on Computational Creativity (ICCC'23)_.
[doi.org/10.5281/zenodo.8087978](https://doi.org/10.5281/zenodo.8087978).

**Vigliensoni, G.**, and R. Fiebrink. 2023. Interacting with neural audio synthesis models through interactive machine learning. In _The First International Workshop on eXplainable AI for the Arts at the ACM Creativity and Cognition Conference (XAIxArts2023)_.

**Vigliensoni, G.**, and R. Fiebrink. 2023. Re•col•lec•tions: Sharing sonic memories through interactive machine learning and neural audio synthesis models. _In Creative AI track of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)_.

Shimizu, J., I. Olowe, T. Broad, **G. Vigliensoni**, P. Thattai, and R. Fiebrink. 2023. Interactive machine learning for generative models. In _Proceedings of the Machine Learning for Creativity and Design Workshop, 37th Conference on Neural Information Processing Systems (NeurIPS 2023)_.

Fujinaga, I, and **G. Vigliensoni**. 2023. Optical music recognition workflow for medieval music manuscripts. In_ Proceedings of the 5th International Workshop on Reading Music Systems (WoRMS 2023)_.



### 2022
**Vigliensoni, G.**, L. McCallum, E. Maestre, and R. Fiebrink. 2022. R-VAE: Live latent space drum rhythm generation from minimal-size datasets. _Journal of Creative Music Systems 1(1)_.  [doi.org/10.5920/jcms.902](https://doi.org/10.5920/jcms.902).

**Vigliensoni, G.**, P. Perry, and R. Fiebrink. 2022. A small-data mindset for generative AI creative work. In Proceedings of the _Generative AI and HCI Workshop - Conference on Human Factors in Computing Systems Workshop (CHI2022)_.  [doi.org/10.5281/zenodo.7086327](https://doi.org/10.5281/zenodo.7086327).


### 2021

**Vigliensoni, G.**, E. de Luca, and I. Fujinaga. 2021. Chapter 6: Repertoire: Neume Notation. In _Music Encoding Initiative Guidelines_, edited by J. Kepper et al.

### 2020

**Vigliensoni, G.**, L. McCallum, E. Maestre, and R. Fiebrink. 2020. Generation and visualization of rhythmic latent spaces. In _Proceedings of the 2020 Joint Conference on AI Music Creativity_.  [doi.org/10.5281/zenodo.4285422](https://doi.org/10.5281/zenodo.4285422)

**Vigliensoni, G.**, L. McCallum, and R. Fiebrink. 2020. Creating latent spaces for modern music genre rhythms using minimal training data. In _Proceedings of the 11th International Conference on Computational Creativity (ICCC’20)_.  [doi.org/10.5281/zenodo.7415792](https://doi.org/10.5281/zenodo.7415792)

**Vigliensoni, G.**, E. Maestre, and R. Fiebrink. 2020. Web-based dynamic visualization of rhythmic latent space. In _Proceedings of the Sound, 
Image and Interaction Design Symposium (SIIDS2020)._  [doi.org/10.5281/zenodo.7438305](https://doi.org/10.5281/zenodo.7438305)

Regimbal, J., **G. Vigliensoni**, C. Hutnik, and I. Fujinaga. 2020. IIIF-based lyric and neume editor for square-notation manuscripts. In _Proceedings of the Music Encoding Conference_.

### 2019

Fujinaga, I., and **G. Vigliensoni** 2019. The Art of Teaching Computers: The SIMSSA Optical Music Recognition Workflow System. In _Proceedings of the 27th European Signal Processing Conference_. [doi.org/10.23919/eusipco.2019.8902658](https://doi.org/10.23919/eusipco.2019.8902658)


**Vigliensoni, G.**, A. Daigle, E. Liu, J. Calvo-Zaragoza, J. Regimbal, M. A. Nguyen, N. Baxter, Z. McLennan, and I. Fujinaga. 2019. Overcoming the challenges of optical music recognition of Early Music with machine learning. _Digital Humanities Conference 2019_.

**Vigliensoni, G.**, A. Daigle, E. Liu, J. Calvo-Zaragoza, J. Regimbal, M. A. Nguyen, N. Baxter, Z. McLennan, and I. Fujinaga. 2019. From image to encoding: Full optical music recognition of Medieval and Renaissance music. _Music Encoding Conference 2019_.

### 2018

**Vigliensoni, G.**, J. Calvo-Zaragoza, and I. Fujinaga. 2018. Developing an environment for teaching computers to read music. In _Proceedings of 1st International Workshop on Reading Music Systems_.

Castellanos, F., J. Calvo-Zaragoza, G. Vigliensoni, and I. Fujinaga. 2018. Document analysis of music score images with selectional auto-encoders. In _Proceedings of the 19th International Society for Music Information Retrieval Conference_.

Nápoles, N., **G. Vigliensoni**, and I. Fujinaga. 2018. Encoding matters. In _Proceedings of the 5th International Conference on Digital Libraries for Musicology_. [doi.org/10.1145/3273024.3273027](https://doi.org/10.1145/3273024.3273027)

Calvo-Zaragoza, J., F. Castellanos, **G. Vigliensoni**, and I. Fujinaga. 2018. Deep neural networks for document processing of music score images. _Applied Sciences_, 8(5), 654. [doi.org/10.3390/app8050654](https://doi.org/10.3390/app8050654)

**Vigliensoni, G.**, J. Calvo-Zaragoza, and I. Fujinaga. 2018. An environment for machine pedagogy: Learning how to teach computers to read music. In _Proceedings of the Intelligent Music Interfaces for Listening and Creation_ workshop. 

### 2017

Vigliensoni G., D. Romblom, M. P. Verge, and C. Guastavino. 2017. Perceptual evaluation of a virtual acoustic room model. The _Journal of the Acoustical Society of America_ 142(4): 2559.

**Vigliensoni, G.** and I. Fujinaga. 2017. The music listening histories dataset. In _Proceedings of the 18th International Society for Music Information Retrieval Conference_.

Calvo-Zaragoza, J., **G. Vigliensoni**, and I. Fujinaga. 2017. One-step detection of background, staff lines, and symbols in medieval music manuscripts with convolutional neural networks. In _Proceedings of the 18th International Society for Music Information Retrieval Conference_.

Barone, M., K. Dacosta, **G. Vigliensoni**, and M. Woolhouse. 2017. GRAIL: Database linking music metadata across artist, release, and track. In _Proceedings of the 4th International Workshop on Digital Libraries for Musicology_. [doi.org/10.1145/3144749.3144760](https://doi.org/10.1145/3144749.3144760)

Calvo-Zaragoza, J., **G. Vigliensoni**, and I. Fujinaga. 2017. Music document layout analysis through machine learning and human feedback. In _Proceedings of the 12th IAPR International Workshop on Graphics Recognition_. [doi.org/10.1109/icdar.2017.259](https://doi.org/10.1109/icdar.2017.259)

Saleh, Z., K. Zhang,  J. Calvo-Zaragoza, **G. Vigliensoni**, and I. Fujinaga. 2017. Pixel.js: Web-based pixel classification correction platform for ground truth creation. In _Proceedings of the 12th IAPR International Workshop on Graphics Recognition_. [doi.org/10.1109/icdar.2017.267](https://doi.org/10.1109/icdar.2017.267)

Calvo-Zaragoza, J., **G. Vigliensoni**, and I. Fujinaga. 2017. Pixelwise classification for music document analysis. In _Proceedings of the 2017 Seventh International Conference on Image Processing Theory, Tools, and Applications_. [doi.org/10.1109/ipta.2017.8310134](https://doi.org/10.1109/ipta.2017.8310134)

Calvo-Zaragoza, J., **G. Vigliensoni**, and I. Fujinaga. 2017. Pixel-wise binarization of musical documents with Convolutional Neural Networks. In _Proceedings of the 15th IAPR Conference on Machine Vision Applications_. [doi.org/10.23919/mva.2017.7986876](https://doi.org/10.23919/mva.2017.7986876)

Calvo-Zaragoza, J., **G. Vigliensoni**, and I. Fujinaga. 2017. Staff-line detection on greyscale images with pixel classification. In _Proceedings of the 8th Iberian Conference on Pattern Recognition and Image Analysis_. [doi.org/10.1007/978-3-319-58838-4_31](https://doi.org/10.1007/978-3-319-58838-4_31)

Barone, M., K. Dacosta, **G. Vigliensoni**, and M. Woolhouse. 2017. GRAIL: A general recorded audio identity linker. Late breaking session _17th International Society for Music Information Retrieval Conference_.

Calvo-Zaragoza, J., **G. Vigliensoni**, and I. Fujinaga. 2017. A unified approach towards automatic recognition of heterogeneous music documents. In _Proceedings of the Music Encoding Conference_.

Calvo-Zaragoza, J., **G. Vigliensoni**, and I. Fujinaga. 2017. A machine learning framework for the categorization of elements in images of musical documents. In _Proceedings of the Third International Conference on Technologies for Music Notation and Representation_.

### 2016

**Vigliensoni, G.** and I. Fujinaga. 2016. Automatic music recommendation systems: Do demographic, profiling, and contextual features improve their performance?. In _Proceedings of the 17th International Society for Music Information Retrieval Conference_.

Calvo-Zaragoza, J., **G. Vigliensoni**, and I. Fujinaga. 2016. Staff-line detection on greyscale images with pixel classification. Late breaking session _17th International Society for Music Information Retrieval Conference_.

Calvo-Zaragoza, J., **G. Vigliensoni**, and I. Fujinaga. 2016. Document analysis for music scores via machine learning. _3rd International Digital Libraries for Musicology workshop_. [doi.org/10.1145/2970044.2970047](https://doi.org/10.1145/2970044.2970047)

### 2015

Fujinaga, I., **G. Vigliensoni**, and H. Knox. 2015. The making of a computerized harpsichord for analysis and training. _International Symposium on Performance Science_.

Barone, M., K. Dacosta, **G. Vigliensoni**, and M. Woolhouse. 2015. GRAIL: A music identity space collection and API. Late breaking session _16th International Society for Music Information Retrieval Conference_.

### 2014

**Vigliensoni, G.**, and I. Fujinaga. 2014. Time-shift normalization and listener profiling in a large dataset of music listening histories. _Fourth annual Seminar on Cognitively Based Music Informatics Research_.

**Vigliensoni, G.**, and I. Fujinaga. 2014. Identifying time zones in a large dataset of music listening logs. In _Proceedings of the International Workshop on Social Media Retrieval and Analysis_. [doi.org/10.1145/2632188.2632203](https://doi.org/10.1145/2632188.2632203)

### 2013

**Vigliensoni, G.**, J. A. Burgoyne, and I. Fujinaga. 2013. Musicbrainz for the world: the Chilean experience. In _Proceedings of the International Society for Music Information Retrieval Conference_. 

**Vigliensoni, G.**, G. Burlet, and I Fujinaga. 2013. Optical measure recognition in common music notation. In _Proceedings of the International Society for Music Information Retrieval Conference_. 

### 2012

**Vigliensoni, G.**, and M. Wanderley. 2012. A quantitative comparison of position trackers for the development of a touch-less musical interface. In _Proceedings of the New Interfaces for Musical Expression Conference_.

Hankinson, A., J. A. Burgoyne, **G. Vigliensoni**, A. Porter, J. Thompson, W. Liu, R. Chiu, and I. Fujinaga. 2012. Digital document image retrieval using optical music recognition. In _Proceedings of the International Society for Music Information Retrieval Conference_.

Hankinson, A., J. A. Burgoyne, **G. Vigliensoni**, and I. Fujinaga. 2012. Creating a large-scale searchable digital collection from printed music materials. In _Proceedings of Advances in Music Information Research_. [doi.org/10.1145/2187980.2188221](https://doi.org/10.1145/2187980.2188221)

### 2011

**Vigliensoni, G.** 2011. Touch-less gestural control of concatenative sound synthesis. Master's thesis, McGill University.

**Vigliensoni, G.**, J. A. Burgoyne, A. Hankinson, and I. Fujinaga. 2011. Automatic pitch detection in printed square notation. In _Proceedings of the International Society for Music Information Retrieval Conference_. 

Hankinson, A., **G. Vigliensoni**, J. A. Burgoyne, and I. Fujinaga. 2011. New tools for optical chant recognition. _International Association of Music Libraries Conference_. 

Burgoyne, J. A., R. Chiu, **G. Vigliensoni**, A. Hankinson, J. Cumming, and I. Fujinaga. 2011. Creating a fully-searchable edition of the Liber Usualis. _Medieval and Renaissance Music Conference_. 

### 2010

**Vigliensoni, G.**, and M. Wanderley. 2010. Soundcatcher: Explorations in audio-looping and time-freezing using an open-air gestural controller. In _Proceedings of the International Computer Music Conference_. 

McKay, C., J. A. Burgoyne, J. Hockman, J. B. L. Smith, **G. Vigliensoni**, and I. Fujinaga. 2010. Evaluating the performance of lyrical features relative to and in combination with audio, symbolic and cultural features. In _Proceedings of the International Society for Music Information Retrieval Conference_. 

**Vigliensoni, G.**, C. McKay, and I. Fujinaga. 2010. Using jWebMiner 2.0 to improve music classification performance by combining different types of features mined from the web. In _Proceedings of the International Society for Music Information Retrieval Conference_. 


 
# Datasets

The Music Listening Histories Dataset (MLHD). 2017. **Vigliensoni, G.**, and I. Fujinaga.

Music Recommendation Dataset (KGRec-music). 2016. Oramas, S., V. C. Ostuni, and **G. Vigliensoni**. Licensed under Creative Commons CC BY-NC 3.0, except 3rd party data.

Sound Recommendation Dataset (KGRec-sound). 2016. Oramas, S., V. C. Ostuni, and **G. Vigliensoni**. Licensed under Creative Commons CC BY-NC 3.0, except 3rd party data.