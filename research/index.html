<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>vigliensoni</title><meta name=description content="Website for music artist and researcher Gabriel Vigliensoni."><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=https://vigliensoni.com/css/bootstrap.min.css><link rel=stylesheet href="//fonts.googleapis.com/css?family=Roboto:400,300,700,400italic"><link rel=stylesheet href=https://vigliensoni.com/css/font-awesome.min.css><link rel=stylesheet href=https://vigliensoni.com/css/owl.carousel.css><link rel=stylesheet href=https://vigliensoni.com/css/owl.theme.css><link href=https://vigliensoni.com/css/style.default.css rel=stylesheet id=theme-stylesheet><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link href=https://vigliensoni.com/css/custom.css rel=stylesheet><link rel="shortcut icon" href=https://vigliensoni.com/img/favicon.png><link href=https://vigliensoni.com/research/index.xml rel=alternate type=application/rss+xml title=vigliensoni></head><body><div id=all><div class=container-fluid><div class="row row-offcanvas row-offcanvas-left"><div id=sidebar class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas"><div class=sidebar-content><h1 class=sidebar-heading><a href=https://vigliensoni.com/>vigliensoni</a></h1><p class=sidebar-p>Music artist and researcher.</p><ul class=sidebar-menu><li><a href=https://vigliensoni.com/categories/featured>Featured</a></li><li><a href=https://vigliensoni.com/portfolio/>Portfolio</a></li><li><a href=https://vigliensoni.com/categories/music>Music</a></li><li><a href=https://vigliensoni.com/research/>Research</a></li><li><a href=https://vigliensoni.com/categories/presentation/>Workshops & talks</a></li><li><a href=https://vigliensoni.com/about/>About</a></li><li><a href=https://vigliensoni.com/contact/>Contact</a></li></ul><p class=social><a href=/contact/ data-animate-hover=pulse class=email title=E-mail><i class="fa fa-envelope"></i></a>
<a href=https://instagram.com/vigliensoni title class="external instagram" title=Instagram><i class="fa fa-instagram"></i></a>
<a href=https://github.com/vigliensoni data-animate-hover=pulse class=external title=GitHub><i class="fa fa-github"></i></a>
<a href=https://www.youtube.com/@vigliensoni/videos data-animate-hover=pulse class=external title=YouTube><i class="fa fa-youtube"></i></a>
<a href=https://facebook.com/vigliensoni data-animate-hover=pulse class="external facebook" title=Facebook><i class="fa fa-facebook"></i></a></p><div class=copyright><p class=credit>&copy;2025 Gabriel Vigliensoni<br><br>Template by <a href=https://bootstrapious.com/free-templates class=external>Bootstrapious.com</a>
& ported to Hugo by <a href=https://github.com/kishaningithub>Kishan B</a></p></div></div></div><div class="col-xs-12 col-sm-8 col-md-9 content-column white-background"><div class="small-navbar visible-xs"><button type=button data-toggle=offcanvas class="btn btn-ghost pull-left"> <i class="fa fa-align-left"></i>Menu</button><h1 class=small-navbar-heading><a href=https://vigliensoni.com/>vigliensoni</a></h1></div><div class=row><div class=col-lg-8><div class=content-column-content><h1>Research</h1><h2 id=statement>Statement</h2><p>A search for the creative possibilities of new technologies has been at the core of my scientific and artistic research in my academic and professional career. In my early graduate research, I combined human-computer interaction, physical computing, and music information retrieval techniques to develop novel gestural interfaces to control sound synthesis processes that I then used expressively in my creative practice. Currently, I am exploring the creative affordances of the machine learning paradigm in music composition and performance. I develop, adapt, compose, and perform with machine learning techniques and tools that learn models from music encoded in a symbolic format. In my current research, I am investigating the expressive possibilities of machine learning-assisted music-making directly in the audio domain.</p><p>The contemporary and rapidly changing new music economy landscape requires researchers, professionals, and makers not only aware of the new tools and techniques to design and implement new creative output, but also to think critically on the biases and implications of using data-driven technologies in creative practice in general, and music-making in particular. As a researcher, scholar, computer musician, and performer, these are for me important topics and a focal point. As a result, in my scientific research and creative output, I interrogate and explore the aesthetic possibilities of new technologies, but I also pay attention to their omissions, silences, and disconnections. Hence, I am committed to an inclusive approach to the development of new ways of producing, researching, and enjoying music.</p><h2 id=publications>Publications</h2><h3 id=2025>2025</h3><p><strong>Vigliensoni, G.</strong>, and R. Fiebrink. 2025. Data- and interaction-driven approaches for sustained musical practices with machine learning. <em>Journal of New Music Research</em>, January, 1–14. <a href=https://doi.org/10.1080/09298215.2024.2442361>doi:10.1080/09298215.2024.2442361</a>.</p><p>Bryan-Kinns, N., S. Zheng, F. Castro, M. Lewis, and J. Chang, <strong>G. Vigliensoni</strong>, T. Broad, M. P. Clemens, and E. Wilson. 2025. XAIxArts Manifesto: Explainable AI for the Arts. In <em>Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</em>. <a href=https://arxiv.org/abs/2502.21220>doi:10.1145/3706599.3716227</a></p><p>Ford, C., E. Wilson, S. Zheng, <strong>G. Vigliensoni</strong>, J. Rezwana, L. Xiao, M. Clemens, M. Lewis, D. Hemment, A. Chamberlain, H. Kennedy, and N. Bryan-Kinns. Explainable AI for the Arts 3 (XAIxArts2). In <em>Proceedings of the ACM Creativity and Cognition Conference (C&C ’25)</em>.</p><p>Nickel, V., and <strong>G. Vigliensoni</strong>. 2025. Sonic lead: A survey of sound-first games. To be published in <em>Proceedings of the Digital Games Research Association (DiGRA)</em>.</p><p>Bryan-Kinns, N., A. Wszeborowska, O. Sutskova, E. Wilson, P. Perry, R. Fiebrink, <strong>G. Vigliensoni</strong>, R. Lindell, A. Coronel, and N. Correia. 2025. Responsible and ethical use of AI for music: The value of small datasets. To be published in <em>Proceedings of AM.ICAD 2025 (Audio Mostly + International Conference on Auditory Display: Joint Conference)</em>.</p><p>Nguyen, V., and <strong>G. Vigliensoni</strong>. 2025. Embedding visual thinking into an AI-driven furniture design critiquing system. To be published in <em>Proceedings of the International Conference on Computational Creativity (ICCC)</em>.</p><h3 id=2024>2024</h3><p>Tecks, A., T. Peschlow, and <strong>G. Vigliensoni</strong>. 2024. Explainability paths for sustained artistic practice. In <em>Proceedings of the the Second International Workshop on eXplainable AI for the Arts at the ACM Creativity and Cognition Conference (XAIxArts2024)</em>. <a href=https://doi.org/10.48550/arxiv.2407.15216>doi.org/10.48550/arxiv.2407.15216</a></p><p>Bryan-Kinns, N., C. Ford, S. Zheng, H. Kennedy, A. Chamberlain, M. Lewis, D. Hemment, Z. Li, Q. Wu, L. Xiao, G. Xia, J. Rezwana, M. Clemens, and <strong>G. Vigliensoni</strong>. Explainable AI for the Arts 2 (XAIxArts2). In <em>Proceedings of the ACM Creativity and Cognition Conference (C&C ’24)</em>. <a href=https://doi.org/10.1145/3635636.3660763>doi.org/10.1145/3635636.3660763</a></p><h3 id=2023>2023</h3><p><strong>Vigliensoni, G.</strong>, and R. Fiebrink. 2023. Steering latent audio models through interactive machine learning. In <em>Proceedings of the 14th International Conference on Computational Creativity (ICCC'23)</em>.
<a href=https://doi.org/10.5281/zenodo.8087978>doi.org/10.5281/zenodo.8087978</a>.</p><p><strong>Vigliensoni, G.</strong>, and R. Fiebrink. 2023. Interacting with neural audio synthesis models through interactive machine learning. In <em>The First International Workshop on eXplainable AI for the Arts at the ACM Creativity and Cognition Conference (XAIxArts2023)</em>.</p><p><strong>Vigliensoni, G.</strong>, and R. Fiebrink. 2023. Re•col•lec•tions: Sharing sonic memories through interactive machine learning and neural audio synthesis models. <em>In Creative AI track of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)</em>.</p><p>Shimizu, J., I. Olowe, T. Broad, <strong>G. Vigliensoni</strong>, P. Thattai, and R. Fiebrink. 2023. Interactive machine learning for generative models. In <em>Proceedings of the Machine Learning for Creativity and Design Workshop, 37th Conference on Neural Information Processing Systems (NeurIPS 2023)</em>.</p><p>Fujinaga, I, and <strong>G. Vigliensoni</strong>. 2023. Optical music recognition workflow for medieval music manuscripts. In_ Proceedings of the 5th International Workshop on Reading Music Systems (WoRMS 2023)_.</p><h3 id=2022>2022</h3><p><strong>Vigliensoni, G.</strong>, L. McCallum, E. Maestre, and R. Fiebrink. 2022. R-VAE: Live latent space drum rhythm generation from minimal-size datasets. <em>Journal of Creative Music Systems 1(1)</em>. <a href=https://doi.org/10.5920/jcms.902>doi.org/10.5920/jcms.902</a>.</p><p><strong>Vigliensoni, G.</strong>, P. Perry, and R. Fiebrink. 2022. A small-data mindset for generative AI creative work. In Proceedings of the <em>Generative AI and HCI Workshop - Conference on Human Factors in Computing Systems Workshop (CHI2022)</em>. <a href=https://doi.org/10.5281/zenodo.7086327>doi.org/10.5281/zenodo.7086327</a>.</p><h3 id=2021>2021</h3><p><strong>Vigliensoni, G.</strong>, E. de Luca, and I. Fujinaga. 2021. Chapter 6: Repertoire: Neume Notation. In <em>Music Encoding Initiative Guidelines</em>, edited by J. Kepper et al.</p><h3 id=2020>2020</h3><p><strong>Vigliensoni, G.</strong>, L. McCallum, E. Maestre, and R. Fiebrink. 2020. Generation and visualization of rhythmic latent spaces. In <em>Proceedings of the 2020 Joint Conference on AI Music Creativity</em>. <a href=https://doi.org/10.5281/zenodo.4285422>doi.org/10.5281/zenodo.4285422</a></p><p><strong>Vigliensoni, G.</strong>, L. McCallum, and R. Fiebrink. 2020. Creating latent spaces for modern music genre rhythms using minimal training data. In <em>Proceedings of the 11th International Conference on Computational Creativity (ICCC’20)</em>. <a href=https://doi.org/10.5281/zenodo.7415792>doi.org/10.5281/zenodo.7415792</a></p><p><strong>Vigliensoni, G.</strong>, E. Maestre, and R. Fiebrink. 2020. Web-based dynamic visualization of rhythmic latent space. In <em>Proceedings of the Sound,
Image and Interaction Design Symposium (SIIDS2020).</em> <a href=https://doi.org/10.5281/zenodo.7438305>doi.org/10.5281/zenodo.7438305</a></p><p>Regimbal, J., <strong>G. Vigliensoni</strong>, C. Hutnik, and I. Fujinaga. 2020. IIIF-based lyric and neume editor for square-notation manuscripts. In <em>Proceedings of the Music Encoding Conference</em>.</p><h3 id=2019>2019</h3><p>Fujinaga, I., and <strong>G. Vigliensoni</strong> 2019. The Art of Teaching Computers: The SIMSSA Optical Music Recognition Workflow System. In <em>Proceedings of the 27th European Signal Processing Conference</em>. <a href=https://doi.org/10.23919/eusipco.2019.8902658>doi.org/10.23919/eusipco.2019.8902658</a></p><p><strong>Vigliensoni, G.</strong>, A. Daigle, E. Liu, J. Calvo-Zaragoza, J. Regimbal, M. A. Nguyen, N. Baxter, Z. McLennan, and I. Fujinaga. 2019. Overcoming the challenges of optical music recognition of Early Music with machine learning. <em>Digital Humanities Conference 2019</em>.</p><p><strong>Vigliensoni, G.</strong>, A. Daigle, E. Liu, J. Calvo-Zaragoza, J. Regimbal, M. A. Nguyen, N. Baxter, Z. McLennan, and I. Fujinaga. 2019. From image to encoding: Full optical music recognition of Medieval and Renaissance music. <em>Music Encoding Conference 2019</em>.</p><h3 id=2018>2018</h3><p><strong>Vigliensoni, G.</strong>, J. Calvo-Zaragoza, and I. Fujinaga. 2018. Developing an environment for teaching computers to read music. In <em>Proceedings of 1st International Workshop on Reading Music Systems</em>.</p><p>Castellanos, F., J. Calvo-Zaragoza, G. Vigliensoni, and I. Fujinaga. 2018. Document analysis of music score images with selectional auto-encoders. In <em>Proceedings of the 19th International Society for Music Information Retrieval Conference</em>.</p><p>Nápoles, N., <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2018. Encoding matters. In <em>Proceedings of the 5th International Conference on Digital Libraries for Musicology</em>. <a href=https://doi.org/10.1145/3273024.3273027>doi.org/10.1145/3273024.3273027</a></p><p>Calvo-Zaragoza, J., F. Castellanos, <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2018. Deep neural networks for document processing of music score images. <em>Applied Sciences</em>, 8(5), 654. <a href=https://doi.org/10.3390/app8050654>doi.org/10.3390/app8050654</a></p><p><strong>Vigliensoni, G.</strong>, J. Calvo-Zaragoza, and I. Fujinaga. 2018. An environment for machine pedagogy: Learning how to teach computers to read music. In <em>Proceedings of the Intelligent Music Interfaces for Listening and Creation</em> workshop.</p><h3 id=2017>2017</h3><p>Vigliensoni G., D. Romblom, M. P. Verge, and C. Guastavino. 2017. Perceptual evaluation of a virtual acoustic room model. The <em>Journal of the Acoustical Society of America</em> 142(4): 2559.</p><p><strong>Vigliensoni, G.</strong> and I. Fujinaga. 2017. The music listening histories dataset. In <em>Proceedings of the 18th International Society for Music Information Retrieval Conference</em>.</p><p>Calvo-Zaragoza, J., <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2017. One-step detection of background, staff lines, and symbols in medieval music manuscripts with convolutional neural networks. In <em>Proceedings of the 18th International Society for Music Information Retrieval Conference</em>.</p><p>Barone, M., K. Dacosta, <strong>G. Vigliensoni</strong>, and M. Woolhouse. 2017. GRAIL: Database linking music metadata across artist, release, and track. In <em>Proceedings of the 4th International Workshop on Digital Libraries for Musicology</em>. <a href=https://doi.org/10.1145/3144749.3144760>doi.org/10.1145/3144749.3144760</a></p><p>Calvo-Zaragoza, J., <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2017. Music document layout analysis through machine learning and human feedback. In <em>Proceedings of the 12th IAPR International Workshop on Graphics Recognition</em>. <a href=https://doi.org/10.1109/icdar.2017.259>doi.org/10.1109/icdar.2017.259</a></p><p>Saleh, Z., K. Zhang, J. Calvo-Zaragoza, <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2017. Pixel.js: Web-based pixel classification correction platform for ground truth creation. In <em>Proceedings of the 12th IAPR International Workshop on Graphics Recognition</em>. <a href=https://doi.org/10.1109/icdar.2017.267>doi.org/10.1109/icdar.2017.267</a></p><p>Calvo-Zaragoza, J., <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2017. Pixelwise classification for music document analysis. In <em>Proceedings of the 2017 Seventh International Conference on Image Processing Theory, Tools, and Applications</em>. <a href=https://doi.org/10.1109/ipta.2017.8310134>doi.org/10.1109/ipta.2017.8310134</a></p><p>Calvo-Zaragoza, J., <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2017. Pixel-wise binarization of musical documents with Convolutional Neural Networks. In <em>Proceedings of the 15th IAPR Conference on Machine Vision Applications</em>. <a href=https://doi.org/10.23919/mva.2017.7986876>doi.org/10.23919/mva.2017.7986876</a></p><p>Calvo-Zaragoza, J., <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2017. Staff-line detection on greyscale images with pixel classification. In <em>Proceedings of the 8th Iberian Conference on Pattern Recognition and Image Analysis</em>. <a href=https://doi.org/10.1007/978-3-319-58838-4_31>doi.org/10.1007/978-3-319-58838-4_31</a></p><p>Barone, M., K. Dacosta, <strong>G. Vigliensoni</strong>, and M. Woolhouse. 2017. GRAIL: A general recorded audio identity linker. Late breaking session <em>17th International Society for Music Information Retrieval Conference</em>.</p><p>Calvo-Zaragoza, J., <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2017. A unified approach towards automatic recognition of heterogeneous music documents. In <em>Proceedings of the Music Encoding Conference</em>.</p><p>Calvo-Zaragoza, J., <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2017. A machine learning framework for the categorization of elements in images of musical documents. In <em>Proceedings of the Third International Conference on Technologies for Music Notation and Representation</em>.</p><h3 id=2016>2016</h3><p><strong>Vigliensoni, G.</strong> and I. Fujinaga. 2016. Automatic music recommendation systems: Do demographic, profiling, and contextual features improve their performance?. In <em>Proceedings of the 17th International Society for Music Information Retrieval Conference</em>.</p><p>Calvo-Zaragoza, J., <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2016. Staff-line detection on greyscale images with pixel classification. Late breaking session <em>17th International Society for Music Information Retrieval Conference</em>.</p><p>Calvo-Zaragoza, J., <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2016. Document analysis for music scores via machine learning. <em>3rd International Digital Libraries for Musicology workshop</em>. <a href=https://doi.org/10.1145/2970044.2970047>doi.org/10.1145/2970044.2970047</a></p><h3 id=2015>2015</h3><p>Fujinaga, I., <strong>G. Vigliensoni</strong>, and H. Knox. 2015. The making of a computerized harpsichord for analysis and training. <em>International Symposium on Performance Science</em>.</p><p>Barone, M., K. Dacosta, <strong>G. Vigliensoni</strong>, and M. Woolhouse. 2015. GRAIL: A music identity space collection and API. Late breaking session <em>16th International Society for Music Information Retrieval Conference</em>.</p><h3 id=2014>2014</h3><p><strong>Vigliensoni, G.</strong>, and I. Fujinaga. 2014. Time-shift normalization and listener profiling in a large dataset of music listening histories. <em>Fourth annual Seminar on Cognitively Based Music Informatics Research</em>.</p><p><strong>Vigliensoni, G.</strong>, and I. Fujinaga. 2014. Identifying time zones in a large dataset of music listening logs. In <em>Proceedings of the International Workshop on Social Media Retrieval and Analysis</em>. <a href=https://doi.org/10.1145/2632188.2632203>doi.org/10.1145/2632188.2632203</a></p><h3 id=2013>2013</h3><p><strong>Vigliensoni, G.</strong>, J. A. Burgoyne, and I. Fujinaga. 2013. Musicbrainz for the world: the Chilean experience. In <em>Proceedings of the International Society for Music Information Retrieval Conference</em>.</p><p><strong>Vigliensoni, G.</strong>, G. Burlet, and I Fujinaga. 2013. Optical measure recognition in common music notation. In <em>Proceedings of the International Society for Music Information Retrieval Conference</em>.</p><h3 id=2012>2012</h3><p><strong>Vigliensoni, G.</strong>, and M. Wanderley. 2012. A quantitative comparison of position trackers for the development of a touch-less musical interface. In <em>Proceedings of the New Interfaces for Musical Expression Conference</em>.</p><p>Hankinson, A., J. A. Burgoyne, <strong>G. Vigliensoni</strong>, A. Porter, J. Thompson, W. Liu, R. Chiu, and I. Fujinaga. 2012. Digital document image retrieval using optical music recognition. In <em>Proceedings of the International Society for Music Information Retrieval Conference</em>.</p><p>Hankinson, A., J. A. Burgoyne, <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2012. Creating a large-scale searchable digital collection from printed music materials. In <em>Proceedings of Advances in Music Information Research</em>. <a href=https://doi.org/10.1145/2187980.2188221>doi.org/10.1145/2187980.2188221</a></p><h3 id=2011>2011</h3><p><strong>Vigliensoni, G.</strong> 2011. Touch-less gestural control of concatenative sound synthesis. Master&rsquo;s thesis, McGill University.</p><p><strong>Vigliensoni, G.</strong>, J. A. Burgoyne, A. Hankinson, and I. Fujinaga. 2011. Automatic pitch detection in printed square notation. In <em>Proceedings of the International Society for Music Information Retrieval Conference</em>.</p><p>Hankinson, A., <strong>G. Vigliensoni</strong>, J. A. Burgoyne, and I. Fujinaga. 2011. New tools for optical chant recognition. <em>International Association of Music Libraries Conference</em>.</p><p>Burgoyne, J. A., R. Chiu, <strong>G. Vigliensoni</strong>, A. Hankinson, J. Cumming, and I. Fujinaga. 2011. Creating a fully-searchable edition of the Liber Usualis. <em>Medieval and Renaissance Music Conference</em>.</p><h3 id=2010>2010</h3><p><strong>Vigliensoni, G.</strong>, and M. Wanderley. 2010. Soundcatcher: Explorations in audio-looping and time-freezing using an open-air gestural controller. In <em>Proceedings of the International Computer Music Conference</em>.</p><p>McKay, C., J. A. Burgoyne, J. Hockman, J. B. L. Smith, <strong>G. Vigliensoni</strong>, and I. Fujinaga. 2010. Evaluating the performance of lyrical features relative to and in combination with audio, symbolic and cultural features. In <em>Proceedings of the International Society for Music Information Retrieval Conference</em>.</p><p><strong>Vigliensoni, G.</strong>, C. McKay, and I. Fujinaga. 2010. Using jWebMiner 2.0 to improve music classification performance by combining different types of features mined from the web. In <em>Proceedings of the International Society for Music Information Retrieval Conference</em>.</p><h1 id=datasets>Datasets</h1><p>The Music Listening Histories Dataset (MLHD). 2017. <strong>Vigliensoni, G.</strong>, and I. Fujinaga.</p><p>Music Recommendation Dataset (KGRec-music). 2016. Oramas, S., V. C. Ostuni, and <strong>G. Vigliensoni</strong>. Licensed under Creative Commons CC BY-NC 3.0, except 3rd party data.</p><p>Sound Recommendation Dataset (KGRec-sound). 2016. Oramas, S., V. C. Ostuni, and <strong>G. Vigliensoni</strong>. Licensed under Creative Commons CC BY-NC 3.0, except 3rd party data.</p></div></div></div></div></div></div></div><script src=https://vigliensoni.com/js/jquery.min.js></script>
<script src=https://vigliensoni.com/js/bootstrap.min.js></script>
<script src=https://vigliensoni.com/js/jquery.cookie.js></script><script src=https://vigliensoni.com/js/ekko-lightbox.js></script>
<script src=https://vigliensoni.com/js/jquery.scrollTo.min.js></script>
<script src=https://vigliensoni.com/js/masonry.pkgd.min.js></script>
<script src=https://vigliensoni.com/js/imagesloaded.pkgd.min.js></script>
<script src=https://vigliensoni.com/js/owl.carousel.min.js></script>
<script src=https://vigliensoni.com/js/front.js></script></body></html>